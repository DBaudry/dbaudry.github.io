[{"authors":null,"categories":null,"content":"I am currently a postdoctoral researcher in the Department of Statistics at the University of Oxford, working with Patrick Rebeschini on online learning and optimization. In July 2025, I will start a new position as a permanent researcher at Inria Grenoble in the new Ghost team (formerly Polaris).\nPrior to that, I was a postdoctoral researcher at the Inria Fairplay team, based at ENSAE Paris, collaborating with Vianney Perchet. My work focused on online algorithms and bandit models, particularly in structured and constrained problem settings, with practical applications in online advertising and recommendation systems.\nI received my PhD in Computer Science from the University of Lille, where I worked under the direction of Emilie Kaufmann and Odalric-Ambrym Maillard in the INRIA ScooL team. My PhD research focused on non-parametric algorithms for the Multi-Armed Bandit problem, motivated by an application in agriculture. I explored several approaches based on sub-sampling or bootstrapping, designed to work under non-parametric assumptions on the reward distributions. I then extended these works to address practical considerations: risk-aware learning, non-stationarity of rewards, and batched feedback. During my PhD, I also had the privilege to visit Pr. Junya Honda at the University of Kyoto for 3 months, where I further explored the connection between different families of bandit algorithms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dorian-baudry.netlify.app/author/dorian-baudry/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dorian-baudry/","section":"authors","summary":"I am currently a postdoctoral researcher in the Department of Statistics at the University of Oxford, working with Patrick Rebeschini on online learning and optimization. In July 2025, I will start a new position as a permanent researcher at Inria Grenoble in the new Ghost team (formerly Polaris).","tags":null,"title":"Dorian Baudry","type":"authors"},{"authors":["Marius Potfer","Dorian Baudry","Hugo Richard","Vianney Perchet","Cheng Wan"],"categories":null,"content":"","date":1733569930,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733569930,"objectID":"faff480880f0547097b5efd373a99f38","permalink":"https://dorian-baudry.netlify.app/publication/auctions/","publishdate":"2024-12-07T12:12:10+01:00","relpermalink":"/publication/auctions/","section":"publication","summary":"Motivated by the strategic participation of electricity producers in electricity day-ahead market, we study the problem of online learning in repeated multi-unit uniform price auctions. The main contribution of this paper is the introduction of a new modelling of the action space. Indeed, we prove that a learning algorithm leveraging the structure of this problem achieves a regret of O(K^{4/3}T^{2/3}) under bandit feedback, improving over the bound previously obtained in the literature. This improved regret rate is tight up to logarithmic terms. Inspired by electricity reserve markets, we further introduce a different feedback model under which all winning bids are revealed. This feedback interpolate between the full-information and bandit scenario depending on the auctions' result. We prove that, under this feedback, the algorithm that we propose achieves regret O(K^{5/2}T^{1/2}).","tags":["Source Themes"],"title":"Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits","type":"publication"},{"authors":["Nadav Merlis","Dorian Baudry","Vianney Perchet"],"categories":null,"content":"","date":1733569920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733569920,"objectID":"5b14ad603dfa99bcfaa09c92a66646a6","permalink":"https://dorian-baudry.netlify.app/publication/lookahead/","publishdate":"2024-12-07T12:12:00+01:00","relpermalink":"/publication/lookahead/","section":"publication","summary":"In reinforcement learning (RL), agents sequentially interact with changing environments while aiming to maximize the obtained rewards. Usually, rewards are observed only after acting, and so the goal is to maximize the expected cumulative reward. Yet, in many practical settings, reward information is observed in advance – prices are observed before performing transactions; nearby traffic information is partially known; and goals are oftentimes given to agents prior to the interaction. In this work, we aim to quantifiably analyze the value of such future reward information through the lens of competitive analysis. In particular, we measure the ratio between the value of standard RL agents and that of agents with partial future-reward lookahead. We characterize the worst-case reward distribution and derive exact ratios for the worst-case reward expectations. Surprisingly, the re12 sulting ratios relate to known quantities in offline RL and reward-free exploration. We further provide tight bounds for the ratio given the worst-case dynamics. Our results cover the full spectrum between observing the immediate rewards before acting to observing all the rewards before the interaction starts.","tags":["Source Themes"],"title":"The Value of Reward Lookahead in Reinforcement Learning","type":"publication"},{"authors":["Ziyad Benomar","Dorian Baudry","Vianney Perchet"],"categories":null,"content":"","date":1733529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733529600,"objectID":"482e588fbc71d015f506a5b617934b36","permalink":"https://dorian-baudry.netlify.app/publication/lookback/","publishdate":"2024-12-07T00:00:00Z","relpermalink":"/publication/lookback/","section":"publication","summary":"Prophet inequalities are fundamental optimal stopping problems, where a decision-maker observes sequentially items with values sampled independently from known distributions, and must decide at each new observation to either stop and gain the current value or reject it irrevocably and move to the next step. This model is often too pessimistic and does not adequately represent real-world online selection processes. Potentially, rejectesd items can be revisited and a fraction of their value can be recovered. To analyze this problem, we consider general decay functions, quantifying the value to be recovered from a rejected item, depending on how far it has been observed in the past. We analyze how lookback improves, or not, the competitive ratio in prophet inequalities in different order models. We show that, under mild monotonicity assumptions on the decay functions, the problem can be reduced to the case where all the decay functions are equal to the same function. Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of the decay rate.","tags":["Source Themes"],"title":"Lookback Prophet Inequalities","type":"publication"},{"authors":["Dorian Baudry","Hugo Richard","Maria Cherifa","Clément Calauzennes","Vianney Perchet"],"categories":null,"content":"","date":1733529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733529600,"objectID":"bb2621ecc56a28ed9249a507ed537c45","permalink":"https://dorian-baudry.netlify.app/publication/coalition/","publishdate":"2024-12-07T00:00:00Z","relpermalink":"/publication/coalition/","section":"publication","summary":"Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function F. In each auction t, a decision-maker bound by limited observations selects nt agents from a coalition of N to compete for a prize with p other agents, aiming to maximize the cumulative reward of the coalition across all auctions. The problem is framed as an N-armed structured bandit, each number of player sent being an arm n, with expected reward r(n) fully characterized by F and p + n. We present two algorithms, Local-Greedy (LG) and Greedy-Grid (GG), both achieving constant problem-dependent regret. This relies on three key ingredients: 1. an estimator of r(n) from feedback collected from any arm k, 2. concentration bounds of these estimates for k within an estimation neighborhood of n and 3. the unimodality property of r under standard assumptions on F. Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees. However, by avoiding to rely on confidence intervals, LG practically outperforms GG, as well as standard unimodal bandit algorithms such as OSUB or multi-armed bandit algorithms.","tags":["Source Themes"],"title":"Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits","type":"publication"},{"authors":["Romain Gautron","Dorian Baudry","Myriam Adam","Gatien N. Falconnier","Gerrit Hoogenboom","Brian King","Marc Corbeels"],"categories":null,"content":"","date":1704107520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704107520,"objectID":"65b6243dd25da00cdf3a4a44106dc57d","permalink":"https://dorian-baudry.netlify.app/publication/fieldcrops/","publishdate":"2024-01-01T12:12:00+01:00","relpermalink":"/publication/fieldcrops/","section":"publication","summary":"We consider a Multi-Armed Bandit problem with covering constraints, where the primary goal is to ensure that each arm receives a minimum expected reward while maximizing the total cumulative reward. In this scenario, the optimal policy then belongs to some unknown feasible set. Unlike much of the existing literature, we do not assume the presence of a safe policy or a feasibility margin, which hinders the exclusive use of conservative approaches. Consequently, we propose and analyze an algorithm that switches between pessimism and optimism in the face of uncertainty. We prove both precise problem-dependent and problemindependent bounds, demonstrating that our algorithm achieves the best of the two approaches – depending on the presence or absence of a feasibility margin – in terms of constraint violation guarantees. Furthermore, our results indicate that playing greedily on the constraints actually outperforms pessimism when considering long-term violations rather than violations on a per-round basis.","tags":["Source Themes"],"title":"A new adaptive identification strategy of best crop management with farmers","type":"publication"},{"authors":["Dorian Baudry","Nadav Merlis","Mathieu Molina","Hugo Richard","Vianney Perchet"],"categories":null,"content":"","date":1704107520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704107520,"objectID":"2888736c0c51b587460f789bc33271d7","permalink":"https://dorian-baudry.netlify.app/publication/spoc/","publishdate":"2024-01-01T12:12:00+01:00","relpermalink":"/publication/spoc/","section":"publication","summary":"We consider a Multi-Armed Bandit problem with covering constraints, where the primary goal is to ensure that each arm receives a minimum expected reward while maximizing the total cumulative reward. In this scenario, the optimal policy then belongs to some unknown feasible set. Unlike much of the existing literature, we do not assume the presence of a safe policy or a feasibility margin, which hinders the exclusive use of conservative approaches. Consequently, we propose and analyze an algorithm that switches between pessimism and optimism in the face of uncertainty. We prove both precise problem-dependent and problemindependent bounds, demonstrating that our algorithm achieves the best of the two approaches – depending on the presence or absence of a feasibility margin – in terms of constraint violation guarantees. Furthermore, our results indicate that playing greedily on the constraints actually outperforms pessimism when considering long-term violations rather than violations on a per-round basis.","tags":["Source Themes"],"title":"Multi-Armed Bandits with Guaranteed Revenue per Arm","type":"publication"},{"authors":["Dorian Baudry","Kazuya Suzuki","Junya Honda"],"categories":null,"content":"","date":1702379520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702379520,"objectID":"0b949dd681f2d58440d919b83f3570f3","permalink":"https://dorian-baudry.netlify.app/publication/med/","publishdate":"2023-12-12T12:12:00+01:00","relpermalink":"/publication/med/","section":"publication","summary":"In this paper we propose a general methodology to derive regret bounds for randomized multi-armed bandit algorithms. It consists in checking a set of sufficient conditions on the sampling probability of each arm and on the family of distributions to prove a logarithmic regret. As a direct application we revisit two famous bandit algorithms, Minimum Empirical Divergence (MED) and Thompson Sampling (TS), under various models for the distributions including single parameter exponential families, Gaussian distributions, bounded distributions, or distributions satisfying some conditions on their moments. In particular, we prove that MED is asymptotically optimal for all these models, but also provide a simple regret analysis of some TS algorithms for which the optimality is already known. We then further illustrate the interest of our approach, by analyzing a new Non-Parametric TS algorithm (h-NPTS), adapted to some families of unbounded reward distributions with a bounded h-moment. This model can for instance capture some non-parametric families of distributions whose variance is upper bounded by a known constant. ","tags":["Source Themes"],"title":"A General Recipe for the Analysis of Randomized Multi-Armed Bandit Algorithms","type":"publication"},{"authors":["Dorian Baudry","Fabien Pesquerel","Rémy Degenne","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1683457920,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683457920,"objectID":"09ae439aac2021bc37cba6ecacb7d25d","permalink":"https://dorian-baudry.netlify.app/publication/fastkinf/","publishdate":"2023-05-07T12:12:00+01:00","relpermalink":"/publication/fastkinf/","section":"publication","summary":"We consider the problem of regret minimization in non-parametric stochastic bandits. When the rewards are known to be bounded from above, there exists asymptotically optimal algorithms, with asymptotic regret depending on an infimum of Kullback-Leibler divergences (KL). These algorithms are computationally expensive and require storing all past rewards, thus simpler but non-optimal algorithms are often used instead. We introduce several methods to approximate the infimum KL which reduce drastically the computational and memory costs of existing optimal algorithms, while keeping their regret guaranties. We apply our findings to design new variants of the MED and IMED algorithms, and demonstrate their interest with extensive numerical simulations.","tags":["Source Themes"],"title":"Fast asymptotically optimal algorithms for non-parametric stochastic bandits","type":"publication"},{"authors":["Dorian Baudry"],"categories":null,"content":"","date":1670670720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670670720,"objectID":"230c1992ff2371a17ea5f7cc9fdc8046","permalink":"https://dorian-baudry.netlify.app/publication/thesis2/","publishdate":"2022-12-10T12:12:00+01:00","relpermalink":"/publication/thesis2/","section":"publication","summary":"A Multi-Armed Bandits (MAB) is a learning problem where an agent sequentially chooses an action among a given set of candidates, collects a reward, and implements a strategy in order to maximize her sum of reward. Motivated by a case study in agriculture, we tackle in this thesis several problems that are relevant towards real-world applications of MAB. The first central question that we considered in this thesis is about the assumptions made on the distributions of rewards. While in theory it is usually convenient to consider simple parametric assumptions (e.g gaussian distributions), the practitioner may have some difficulty to find the right model fitting their problem. For this reason, we analyze two families of nonparametric algorithms, in the sense that they do not require strong parametric assumptions on the distributions for their implementation. We show that these two approaches can achieve strong theoretical guarantees in the standard bandit setting, improving what should be known in advance by the learner compared with previous algorithms. Then, we analyze some extensions of these algorithms that make them more suitable for some real-world applications. A second focus of our work is to consider alternative performance metrics, that may be more suitable than the expected sum of rewards for the practitioner. We propose a risk-aware algorithm for a bandit problem where the learner wants to find a safe arm according to a risk metric: the Conditional-Value-at-Risk. We also propose efficient algorithms for a problem analogous to the limit case of this setting, known as Extreme Bandits. Finally, we also adapt some of our approaches for standard variant of MAB, including one with non-stationary rewards and one with feedback grouped into batches of observations.","tags":["Source Themes"],"title":"Non-parametric algorithms for Multi-Armed Bandits","type":"publication"},{"authors":["Marc Jourdan","Rémy Degenne","Dorian Baudry","Rianne De Heide","Emilie Kaufmann"],"categories":null,"content":"","date":1657192320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657192320,"objectID":"1421cbc96765250c372c3ce180213406","permalink":"https://dorian-baudry.netlify.app/publication/toptwo/","publishdate":"2022-07-07T12:12:00+01:00","relpermalink":"/publication/toptwo/","section":"publication","summary":" Top Two algorithms arose as an adaptation of Thompson sampling to best arm identification in multi-armed bandit models (Russo, 2016), for parametric families of arms. They select the next arm to sample from by randomizing among two candidate arms, a leader and a challenger. Despite their good empirical performance, theoretical guarantees for fixed-confidence best arm identification have only been obtained when the arms are Gaussian with known variances. In this paper, we provide a general analysis of Top Two methods, which identifies desirable properties of the leader, the challenger, and the (possibly non-parametric) distributions of the arms. As a result, we obtain theoretically supported Top Two algorithms for best arm identification with bounded distributions. Our proof method demonstrates in particular that the sampling step used to select the leader inherited from Thompson sampling can be replaced by other choices, like selecting the empirical best arm.","tags":["Source Themes"],"title":"Top Two Algorithms Revisited","type":"publication"},{"authors":["Dorian Baudry","Yoan Russac","Emilie Kaufmann"],"categories":null,"content":"","date":1648379520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648379520,"objectID":"b10ff7aae99ab7992ace3245f8aeeea9","permalink":"https://dorian-baudry.netlify.app/publication/extreme/","publishdate":"2022-03-27T12:12:00+01:00","relpermalink":"/publication/extreme/","section":"publication","summary":"In this paper, we contribute to the Extreme Bandit problem, a variant of Multi-Armed Bandits in which the learner seeks to collect the largest possible reward. We first study the concentration of the maximum of i.i.d random variables under mild assumptions on the tail of the rewards distributions. This analysis motivates the introduction of Quantile of Maxima (QoMax). The properties of QoMax are sufficient to build an Explore-Then-Commit (ETC) strategy, QoMax-ETC, achieving strong asymptotic guarantees despite its simplicity. We then propose and analyze a more adaptive, anytime algorithm, QoMax-SDA, which combines QoMax with a subsampling method recently introduced by Baudry et al. (2021). Both algorithms are more efficient than existing approaches in two aspects (1) they lead to better empirical performance (2) they enjoy a significant reduction of the memory and time complexities.","tags":["Source Themes"],"title":"Efficient Algorithms for Extreme Bandits","type":"publication"},{"authors":["Dorian Baudry","Patrick Saux","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1638875520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638875520,"objectID":"442c1fe57f8d822d191eed1433256d2f","permalink":"https://dorian-baudry.netlify.app/publication/ds/","publishdate":"2021-12-07T12:12:00+01:00","relpermalink":"/publication/ds/","section":"publication","summary":"The stochastic multi-arm bandit problem has been extensively studied under standard assumptions on the arm's distribution (e.g bounded with known support, exponential family, etc). These assumptions are suitable for many real-world problems but sometimes they require knowledge (on tails for instance) that may not be precisely accessible to the practitioner, raising the question of the robustness of bandit algorithms to model misspecification. In this paper we study a generic Dirichlet Sampling (DS) algorithm, based on pairwise comparisons of empirical indices computed with re-sampling of the arms' observations and a data-dependent exploration bonus. We show that different variants of this strategy achieve provably optimal regret guarantees when the distributions are bounded and logarithmic regret for semi-bounded distributions with a mild quantile condition. We also show that a simple tuning achieve robustness with respect to a large class of unbounded distributions, at the cost of slightly worse than logarithmic asymptotic regret. We finally provide numerical experiments showing the merits of DS in a decision-making problem on synthetic agriculture data.","tags":["Source Themes"],"title":"From Optimality to Robustness: Adaptive Re-Sampling Strategies in Stochastic Bandits","type":"publication"},{"authors":["Dorian Baudry","Yoan Russac","Olivier Cappé"],"categories":null,"content":"","date":1612915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612915200,"objectID":"d7cdabccea55e0b7221a012bc47f52ce","permalink":"https://dorian-baudry.netlify.app/publication/lb_sda/","publishdate":"2021-02-10T00:00:00Z","relpermalink":"/publication/lb_sda/","section":"publication","summary":"There has been a recent surge of interest in nonparametric bandit algorithms based on subsampling. One drawback however of these approaches is the additional complexity required by random subsampling and the storage of the full history of rewards. Our first contribution is to show that a simple deterministic subsampling rule, proposed in the recent work of Baudry et al. (2020) under the name of ''last-block subsampling'', is asymptotically optimal in one-parameter exponential families. In addition, we prove that these guarantees also hold when limiting the algorithm memory to a polylogarithmic function of the time horizon. These findings open up new perspectives, in particular for non-stationary scenarios in which the arm distributions evolve over time. We propose a variant of the algorithm in which only the most recent observations are used for subsampling, achieving optimal regret guarantees under the assumption of a known number of abrupt changes. Extensive numerical simulations highlight the merits of this approach, particularly when the changes are not only affecting the means of the rewards.","tags":["Source Themes"],"title":"On Limited-Memory Subsampling Strategies for Bandits","type":"publication"},{"authors":["Dorian Baudry","Romain Gautron","Emilie Kaufmann","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"dfa8ce08addb1394f159c35cfb4041b3","permalink":"https://dorian-baudry.netlify.app/publication/ts_cvar/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/ts_cvar/","section":"publication","summary":"In this paper we study a multi-arm bandit problem in which the quality of each arm is measured by the Conditional Value at Risk (CVaR) at some level alpha of the reward distribution. While existing works in this setting mainly focus on Upper Confidence Bound algorithms, we introduce a new Thompson Sampling approach for CVaR bandits on bounded rewards that is flexible enough to solve a variety of problems grounded on physical resources. Building on a recent work by Riou \u0026 Honda (2020), we introduce B-CVTS for continuous bounded rewards and M-CVTS for multinomial distributions. On the theoretical side, we provide a non-trivial extension of their analysis that enables to theoretically bound their CVaR regret minimization performance. Strikingly, our results show that these strategies are the first to provably achieve asymptotic optimality in CVaR bandits, matching the corresponding asymptotic lower bounds for this setting. Further, we illustrate empirically the benefit of Thompson Sampling approaches both in a realistic environment simulating a use-case in agriculture and on various synthetic examples.","tags":["Source Themes"],"title":"Optimal Thompson Sampling strategies for support-aware CVaR bandits","type":"publication"},{"authors":["Dorian Baudry","Emilie Kaufmann","Odalric-Ambrym Maillard"],"categories":null,"content":"","date":1603365120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603365120,"objectID":"e3cf8ead8905255109df35f6d668cbb3","permalink":"https://dorian-baudry.netlify.app/publication/sub-sampling/","publishdate":"2020-10-22T12:12:00+01:00","relpermalink":"/publication/sub-sampling/","section":"publication","summary":"In this paper we propose the first multi-armed bandit algorithm based on re-sampling that achieves asymptotically optimal regret simultaneously for different families of arms (namely Bernoulli, Gaussian and Poisson distributions). Unlike Thompson Sampling which requires to specify a different prior to be optimal in each case, our proposal RB-SDA does not need any distribution-dependent tuning. RB-SDA belongs to the family of Sub-sampling Duelling Algorithms (SDA) which combines the sub-sampling idea first used by the BESA [1] and SSMC [2] algorithms with different sub-sampling schemes. In particular, RB-SDA uses Random Block sampling. We perform an experimental study assessing the flexibility and robustness of this promising novel approach for exploration in bandit models.","tags":["Source Themes"],"title":"Sub-Sampling Algorithms for Efficient Non-Parametric Bandit Exploration","type":"publication"}]